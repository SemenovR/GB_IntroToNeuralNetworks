{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 5. Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ.</li>\n",
    "    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n",
    "    <li>* Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM</li>\n",
    "    <li>* Предложите свои варианты решения проблемы исчезающего градиента в RNN</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронная сеть работающая с датасетом imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n",
      "Построение модели...\n",
      "Процесс обучения...\n",
      "Epoch 1/2\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4136 - accuracy: 0.8120 - val_loss: 0.3403 - val_accuracy: 0.8538\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.2743 - accuracy: 0.8910 - val_loss: 0.3037 - val_accuracy: 0.8740\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.3037 - accuracy: 0.8740\n",
      "Результат при тестировании: 0.3037417531013489\n",
      "Тестовая точность: 0.8740000128746033\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.random.set_seed(48)\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "maxlen = 200\n",
    "batch_size = 50 # увеличьте значение для ускорения обучения\n",
    "\n",
    "print('Загрузка данных...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'тренировочные последовательности')\n",
    "print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "print('Pad последовательности (примеров в x единицу времени)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Построение модели...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0, recurrent_dropout=0))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Процесс обучения...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2, # увеличьте при необходимости\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Результат при тестировании:', score)\n",
    "print('Тестовая точность:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронная сеть генерирующая текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построчное чтение из примера с текстом \n",
    "#with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n",
    "with open(\"SSSR_anekdot.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"utf-8\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxx Партия-бублик, народу-дырка от бублика! Это и есть советская республика! -\\xa0 Что такое РСФСР? -\\xa0 Редкий случай феноменального сумасшествия России. –\\xa0Скажите, вы коммунист? –\\xa0Нет, я сочувствующий. Но помочь ничем не могу! Муж возвращается с работы: –\\xa0Маша, я в партию вступил! Жена ворчит: –\\xa0Вечно ты во что-нибудь вступаешь!.. -Что такое РКП/б/? -\\xa0 Россия кончит погромом. –АВКП/б/? -\\xa0 Все кончится погромом. -\\xa0 Ну, а \"б\" в скобках? -\\xa0 Большим погромом! Крестьянин приехал в город, заходит в магазин и спрашивает: –\\xa0Нет ли у вас вожжей ? Продавец, не расслышав, показывает тому портреты вождей. Крестьянин замахал руками: –\\xa0Да нет, мне нужны настоящие, крепкие! хх Коммунизм – это советская власть плюс электрификация. Отсюда следует: советская власть – это коммунизм минус электрификация. Или : электрификация – это коммунизм минус советская власть. Из репродуктора доносится: \" Великая Октябрьская революция навеки освободила народ от цепей капитализма\". Старушка – внуку: –\\xa0Точно , так и было !'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалим неинформативные строки\n",
    "text = text.replace(\" xxx\", \"\") \n",
    "text = text.replace(\" ххх\", \"\") \n",
    "text = text.replace(\"  хх\", \"\") \n",
    "text = text.replace(\"   х\", \"\") \n",
    "text = text.replace(\" XXX\", \"\") \n",
    "text = text.replace(\"  - \", \"\") \n",
    "text = text.replace(\"  – \", \"\") \n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/10\n",
      "227/227 [==============================] - 8s 33ms/step - loss: 2.8342\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 2.2314\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 1.9257\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 1.7113\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 7s 33ms/step - loss: 1.5559\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 1.4253\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 1.3046\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 1.1825\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 7s 33ms/step - loss: 1.0604\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.9392\n",
      "Генерация из посева: звыходного\n",
      "звыходного разных сломах из СССР. -  А что это вы можете советские вожди, который бог после прихода в своих коммунизме разворуют в постовой будке и выводят из СССР спросил его в политически предлагает ему будет==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.8170\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.7012\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.5962\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.5043\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.4279\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.3713\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 0.3284\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.2980\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.2753\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.2619\n",
      "Генерация из посева: , а мяса н\n",
      ", а мяса нет в соседнем отделе. Американец, – я прошу вас немедленно востановить Рабинович пришел к Ребе. – Ребе, я к тебе за социализм в отдельно взятой стране ? – Построить-то можно – жить нельзя! Я же съем в==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/10\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 0.2483\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 0.2410\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 0.2366\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.2349\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.2303\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.2282\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.2252\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.2239\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.2217\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.2211\n",
      "Генерация из посева:  такая мал\n",
      " такая маленькая юбка. –А миникомпьютер? -  Это когда все спят! – В это время празднеств по поводу трехсотлетия воссоединения Украины с Россией , не успел надеть штаны: так и пошел в одном исподтерке отделение \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.random.set_seed(48)\n",
    "\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапараметров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 1128, 512 #128, 128\n",
    "NUM_ITERATIONS = 3 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 10 #1\n",
    "NUM_PREDS_PER_EPOCH = 200 #100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. \n",
    "За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в \n",
    "вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию \n",
    "ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=True,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        #dropout=0.2, \n",
    "        #unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=True,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        #dropout=0.2, \n",
    "        #unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        #dropout=0.2, \n",
    "        #unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\") #optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. В отличие от предыдущего ДЗ увеличение количества эпох в данном примере с датасетом imdb не привело к положительному результату. Напротив, даже при небольшом увеличении эпох (до 2-3) возникает переобучение модели. \n",
    "Изменение оптимизатора, простое увеличение числа нейронов в слое и добавление новых слоёв LSTM, так же не привело \n",
    "к значимому эффекту. Попытка бороться с переобучением с помощью увеличения dropout и recurrent_dropout так же окончилась неудачно. Зато обнуление параметров dropout и recurrent_dropout слегка увеличило точность.\n",
    "Так же помогло увеличить точность изменение параметров обучаемого текста. В частности: увеличение длины текста \n",
    "maxlen с 80 до 200 слов и уменьшение max_features с 20000 до 5000. Плюс слегка увеличил число эпох с 1 до 2. \n",
    "В результате точность увеличилась с до 0.8342 до 0.8740.\n",
    "<br>\n",
    "2. Для генерации текста использовал собрание анекдотов СССР. Для того чтобы анектод получался более законченный увеличил длину генерируемого текста до 200 символов. Так же пришлось сделать предобработку текста, удалив неинформативные часто повторяющиеся символы, т.к. модель очень сильно за них \"цеплялась\". Сначала текст получался состоящим из часто повторяющихся слов. Увеличил число итераций до 3, число эпох до 10, число слоёв GRU до 3-х и число нейронов в слоях до 512 - это значительно улучшило текст, текст получился более-менее осмысленный. Так же для увеличения скорости обучения увеличил BATCH_SIZE до 1128. Оценивать задание с генерацией текста оказалось сложнее, в том смысле, что несмотря на минимальную ошибку (loss), понятность генерируемого текста всё равно приходится оценивать визуально.<br>\n",
    "Самый лучший сгенерированный текст:<br>\n",
    "такая маленькая юбка. –А миникомпьютер? -  Это когда все спят! – В это время празднеств по поводу трехсотлетия воссоединения Украины с Россией , не успел надеть штаны: так и пошел в одном исподтерке отделение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
